{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n",
      "/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.layers import SpatialDropout1D,Bidirectional,Dense,Dropout,concatenate,Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D,Input\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import GRU,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")\n",
    "test_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\")\n",
    "sub_df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df)\n",
    "\n",
    "features = train_df[['caps_vs_length','words_vs_unique']].fillna(0)\n",
    "test_features = test_df[['caps_vs_length','words_vs_unique']].fillna(0)\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "ss.fit(np.vstack((features,test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    if file == '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec':\n",
    "        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(crawl)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_glove = load_embed(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose=True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable=(not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:03<00:00, 50472.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "sentences = train_df['comment_text'].apply(lambda x : x.split()).values\n",
    "sentences_test = test_df['comment_text'].apply(lambda x : x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "        \n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_x\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:03<00:00, 51591.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].apply(lambda x : replace_contractions(x))\n",
    "sentences = train_df['comment_text'].apply(lambda x : x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531781/531781 [00:00<00:00, 626529.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 27.55% of vocab\n",
      "Found embeddings for  88.26% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comment_text'] = test_df['comment_text'].apply(lambda x : replace_contractions(x))\n",
    "sentences_test = test_df['comment_text'].apply(lambda x : x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£', \n",
    " 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', \n",
    " 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', \n",
    " 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', \n",
    " 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš','â“’','Â´','â˜¼','â˜º','â™£','â™ ',\n",
    "  'Ï‰', 'âˆ‡âˆ†âˆ‡âˆ†','ð’³','â‚ª','Â«', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text2(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:02<00:00, 54663.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].apply(lambda x : clean_text2(x))\n",
    "sentences = train_df[\"comment_text\"].apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232261/232261 [00:00<00:00, 619957.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.49% of vocab\n",
      "Found embeddings for  98.78% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comment_text'] = test_df['comment_text'].apply(lambda x : clean_text2(x))\n",
    "sentences_test = test_df[\"comment_text\"].apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:02<00:00, 55269.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"comment_text\"] = train_df[\"comment_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train_df[\"comment_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232224/232224 [00:00<00:00, 654544.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.48% of vocab\n",
      "Found embeddings for  98.78% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"comment_text\"] = test_df[\"comment_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "sentences_test = test_df[\"comment_text\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {'MOTHJER':'mother','BeCauSe':'because','DENEID':'denied','Bastered':'bastard','peNis':'penis','AIDSAIDS':'AIDS','HAAHHAHAHAH':'laugh','PenIS':'penis','pneis':'penis','pennnis':'penis','ahahahahahahahahahahahahahahahahahahaha' : 'laugh'\n",
    "            ,'POLITCAL': 'political','FooL':'fool','PaTHeTiC':'pathetic','ReSPeCT':'respect','hellor':'hello','AfDs':'AIDS','pensnsnnienSNsn':'penis','F5FFFA': ' ','\\u200e': ' ','â˜Ž': 'telephone','âœ‰':'mail','2014\\u200e':' ','VANDALIZING':'vandalising'\n",
    "            ,'DIREKTOR': 'director','REVERTING':'revert','Hipocrite':'hypocrite'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:02<00:00, 55237.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for key,value in oov_words.items():\n",
    "    #print(key)\n",
    "    train_df['comment_text'] = train_df['comment_text'].apply( lambda x : x.replace(key,value))\n",
    "    test_df['comment_text'] = test_df['comment_text'].apply( lambda x : x.replace(key,value))\n",
    "    \n",
    "sentences = train_df[\"comment_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232048/232048 [00:00<00:00, 648437.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.54% of vocab\n",
      "Found embeddings for  98.82% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            #M.append(embeddings_index[w])\n",
    "            M.append(embed_glove[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,ytrain,yval = train_test_split(train_df.comment_text.str.lower().values,train_y,random_state=42,shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_tr,feat_val,ytrain_a,yval_a = train_test_split(features,train_y,random_state=42,shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain_glove = [ sent2vec(x) for x in tqdm(xtrain)]\n",
    "#xval_glove = [ sent2vec(x) for x in tqdm(xval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = test_df.comment_text.str.lower().values\n",
    "token = text.Tokenizer(num_words=100000,lower=True)\n",
    "max_len = 150\n",
    "\n",
    "token.fit_on_texts(list(xtrain)+list(xval)+list(xtest))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xval_seq = token.texts_to_sequences(xval)\n",
    "xtest_seq = token.texts_to_sequences(xtest)\n",
    "\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq,maxlen=max_len)\n",
    "xval_pad = sequence.pad_sequences(xval_seq,maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq,maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 356122/356122 [00:01<00:00, 317769.80it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    embedding_vector = embed_glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_inputs = Input(shape=(features.shape[1], ))\n",
    "sequence_inputs = Input(shape=(max_len, ))\n",
    "x = Embedding(len(word_index)+1,300,weights=[embedding_matrix],trainable=False)(sequence_inputs)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "#x = Bidirectional(LSTM(128,return_sequences=True,recurrent_dropout=0.1,dropout=0.1))(x)\n",
    "#x = Bidirectional(LSTM(64,return_sequences=True,recurrent_dropout=0.1,dropout=0.1))(x)\n",
    "#x = Conv1D(64,kernel_size=3,padding=\"valid\",kernel_initializer = \"glorot_uniform\")(x)\n",
    "x = Bidirectional(LSTM(40, return_sequences=True))(x)\n",
    "x, x_h, x_c = Bidirectional(GRU(40, return_sequences=True, return_state = True))(x)  \n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool,x_h,max_pool,feat_inputs])\n",
    "#x = Dense(128,activation='relu')(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "preds = Dense(6,activation='sigmoid')(x)\n",
    "\n",
    "model = Model([sequence_inputs,feat_inputs],preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',patience=5,min_delta=0,verbose=0,mode='max')\n",
    "\n",
    "filepath = \"weights_base.best.hdf5\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath,monitor='val_accuracy',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/20\n",
      "127656/127656 [==============================] - 626s 5ms/step - loss: 0.0642 - accuracy: 0.9782 - val_loss: 0.0451 - val_accuracy: 0.9828\n",
      "Epoch 2/20\n",
      "127656/127656 [==============================] - 625s 5ms/step - loss: 0.0440 - accuracy: 0.9832 - val_loss: 0.0419 - val_accuracy: 0.9835\n",
      "Epoch 3/20\n",
      "127656/127656 [==============================] - 615s 5ms/step - loss: 0.0406 - accuracy: 0.9843 - val_loss: 0.0403 - val_accuracy: 0.9843\n",
      "Epoch 4/20\n",
      "127656/127656 [==============================] - 611s 5ms/step - loss: 0.0388 - accuracy: 0.9848 - val_loss: 0.0395 - val_accuracy: 0.9846\n",
      "Epoch 5/20\n",
      "127656/127656 [==============================] - 611s 5ms/step - loss: 0.0374 - accuracy: 0.9852 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
      "Epoch 6/20\n",
      "127656/127656 [==============================] - 617s 5ms/step - loss: 0.0362 - accuracy: 0.9858 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
      "Epoch 7/20\n",
      "127656/127656 [==============================] - 620s 5ms/step - loss: 0.0349 - accuracy: 0.9860 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
      "Epoch 8/20\n",
      "127656/127656 [==============================] - 620s 5ms/step - loss: 0.0338 - accuracy: 0.9865 - val_loss: 0.0406 - val_accuracy: 0.9844\n",
      "Epoch 9/20\n",
      "127656/127656 [==============================] - 616s 5ms/step - loss: 0.0327 - accuracy: 0.9869 - val_loss: 0.0404 - val_accuracy: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff341ddbda0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([xtrain_pad,feat_tr],y=ytrain,epochs=20,batch_size=128,verbose=1,validation_data=([xval_pad,feat_val],yval),callbacks=[model_checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 17s 108us/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "\n",
    "preds_all = model.predict([xtest_pad,test_features],batch_size=1024,verbose=1)\n",
    "\n",
    "sub_df[list_classes] = preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission_lstm_con.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
